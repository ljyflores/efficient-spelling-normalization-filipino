{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":141,"status":"ok","timestamp":1656107530349,"user":{"displayName":"Lj Flores","userId":"11095487892395270199"},"user_tz":240},"id":"HQIkyRaK17As","outputId":"83561dc4-742b-46d2-fca2-88b3544686cb"},"outputs":[{"name":"stderr","output_type":"stream","text":["2022-07-10 23:58:41.329816: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"]}],"source":["import pandas as pd\n","import numpy as np\n","from utils import *\n","import ast\n","\n","SOURCE_PATH = \"\""]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":182,"status":"ok","timestamp":1656125176378,"user":{"displayName":"Lj Flores","userId":"11095487892395270199"},"user_tz":240},"id":"rStFLkiS-YMb"},"outputs":[],"source":["# Load train/test datasets\n","df      = pd.read_csv(SOURCE_PATH + \"data/train_words.csv\", header=None)\n","df_test = pd.read_csv(SOURCE_PATH + \"data/test_words.csv\", header=None)\n","\n","# Load vocab\n","f = open(\"TagalogStemmerPython/output/with_info.txt\", \"r\", encoding='latin1')\n","f = f.readlines()\n","vocab_tl = set(ast.literal_eval(item.strip('\\n'))['word'] for item in f)\n","vocab_tl = set(df[1]).union(vocab_tl) # Add in vocab from dataframe\n","vocab_tl = set(df_test[1]).union(vocab_tl) # Add in vocab from test dataframe"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# Generate rules\n","og_dict = {}\n","for i in range(df.shape[0]):\n","    og_dict = collate_dict(og_dict, collect_rules(df[0][i], df[1][i], 2))\n","\n","for key in og_dict:\n","    og_dict[key] = og_dict[key]+[key]\n","\n","# Rank rules by frequency\n","og_dict = collate_max(og_dict, 'test')"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Average Time: 2.5701652789115905\n"]},{"data":{"text/plain":["{'best_dl': 0.46,\n"," 'max_dl': 4.73,\n"," 'avg_dl': 2.9108333333333345,\n"," 'acc_1': 0.77,\n"," 'acc_3': 0.82,\n"," 'acc_5': 0.85,\n"," 'target_in_candidate': 0.85}"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["result_lst = generate(word_lst=df_test[0], \n","         rule_dict=og_dict, \n","         vocab=vocab_tl, \n","         use_dld=True,\n","         max_sub=2)\n","evaluate(result_lst, df_test[1])"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Average Time: 2.4510830187797548\n"]},{"data":{"text/plain":["{'best_dl': 1.22,\n"," 'max_dl': 5.29,\n"," 'avg_dl': 3.504833333333332,\n"," 'acc_1': 0.17,\n"," 'acc_3': 0.39,\n"," 'acc_5': 0.58,\n"," 'target_in_candidate': 0.58}"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["result_lst = generate(word_lst=df_test[0], \n","         rule_dict=og_dict, \n","         vocab=vocab_tl, \n","         use_dld=False,\n","         max_sub=2)\n","evaluate(result_lst, df_test[1])"]},{"cell_type":"markdown","metadata":{},"source":["Error Analysis"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# Identify observations where target was not first choice\n","error_idx = [i for (i,(lst,target)) in enumerate(zip(result_lst,df_test[1])) if target!=lst[0]]\n","df_error = df_test.loc[error_idx].reset_index(drop=True)\n","\n","# Generate candidates\n","df_error['Candidates'] = df_error[0].apply(lambda x: generate_candidates(x, og_dict, 2))\n","df_error['In_Candidates'] = [r in cand for (r,cand) in zip(df_error[1], df_error['Candidates'])]\n","\n","# Get score of target\n","df_error['Target_Score']  = [cand[target] if target in cand \\\n","                             else None for (cand, target) \\\n","                             in zip(df_error['Candidates'], df_error[1])]"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["(8.571428571428571, 12.659164700352214)"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# Percentile of target among candidates\n","cand_percentile = [100*sorted(cand.values(), reverse=True).index(target)/len(cand.values()) \\\n","                   for (target,cand) in zip(df_error.loc[df_error.In_Candidates==True]['Target_Score'],\n","                                            df_error.loc[df_error.In_Candidates==True]['Candidates'])]\n","np.median(cand_percentile), np.mean(cand_percentile)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["100.0\n","39.39393939393939\n"]}],"source":["# Generate rules for those not among candidates\n","df_error_no_cand = df_error.loc[df_error.In_Candidates==False].reset_index(drop=True)\n","\n","# Collect rules involved in \n","all_rules_count, missing_rules_count = 0, 0\n","good_rules_lst, bad_rules_lst = [], []\n","\n","for (orig, correct) in zip(df_error_no_cand[0], df_error_no_cand[1]):\n","    # Extract rules which are and are not found in og_dict\n","    good_rules, bad_rules = compare_rules(orig, correct, og_dict)\n","    \n","    # Count number of observations where all rules are present/not\n","    if len(bad_rules)>0:\n","        missing_rules_count += 1\n","    else:\n","        all_rules_count += 1\n","        \n","    # Collect rules across observations\n","    good_rules_lst.extend(good_rules) \n","    bad_rules_lst.extend(bad_rules)\n","    \n","# Percent of observations missing one rule needed to make correction\n","print(100*missing_rules_count/(missing_rules_count+all_rules_count))\n","\n","# Percent of rules missing from the og_dict\n","print(100*len(set(bad_rules_lst))/(len(set(good_rules_lst))+len(set(bad_rules_lst))))"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# Generate rules, now using the original and error words\n","og_dict = {}\n","for i in range(df.shape[0]):\n","    og_dict = collate_dict(og_dict, collect_rules(df[0][i], df[1][i], 2))\n","\n","for i in range(df_error_no_cand.shape[0]):\n","    og_dict = collate_dict(og_dict, collect_rules(df_error_no_cand[0][i], \n","                                                  df_error_no_cand[1][i], \n","                                                  2))\n","for key in og_dict:\n","    og_dict[key] = og_dict[key]+[key]\n","\n","# Rank rules by frequency\n","og_dict = collate_max(og_dict, 'test')"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["2mama tumama ['kama', 'tumama', 'kamo', 'tomo ama', 'tomo']\n","kb ka ba ['ka', 'kaba']\n","na2wa natuwa ['nawa', 'naawa', 'nakaw', 'natuwa', 'nawa']\n","nde hindi ['ng', 'na', 'hinde', 'on', 'hindi']\n","sya siya ['sya', 'saya', 'sa', 'siya', 'si']\n"]}],"source":["# Check which words are still not solved by the updated model\n","result_lst_no_cand = generate(word_lst=df_error_no_cand[0], \n","                              rule_dict=og_dict, \n","                              vocab=vocab_tl, \n","                              use_dld=True,\n","                              max_sub=2)\n","for (orig, target, result) in zip(df_error_no_cand[0],\n","                                  df_error_no_cand[1],\n","                                  result_lst_no_cand):\n","    if target != result[0]:\n","        print(orig, target, result)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNWQnRm+uqiuAInYaP0af/M","collapsed_sections":[],"mount_file_id":"1hWEbrphNNAgInXXHP2Lm4F1-gQlsT5nF","name":"main.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.9.5 ('base': conda)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"},"vscode":{"interpreter":{"hash":"0d78cf5354c54536ee2fe2974b55665bb4fd5d446126f0c5d0792c4750b1da66"}}},"nbformat":4,"nbformat_minor":0}
